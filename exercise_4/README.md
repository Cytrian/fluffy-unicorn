# Exercise 4

In this exercise we will start a Debian VM which acts as the [Matchbox](https://coreos.com/matchbox/docs/latest) and [DHCP server](https://www.isc.org/downloads/dhcp). All the configuration of Matchbox and the other components is done in a static manner. The provisioning scripts for the Matchbox VM can be found in `./scripts`. Like in exercise 1.

Additionally we will start an [etcd](https://coreos.com/etcd/) "cluster" on the Matchbox and we will use [confd](https://github.com/kelseyhightower/confd) to dynamically configure our DHCP server. Also we are starting [Vault](https://www.vaultproject.io) as our PKI and secret store.

The configuration for our machines is stored in two yaml files under `./data` normally these information would be stored in your asset management tool.

## Where to look at

In the following folders you can find important configurations:

- `./data` this contains the "assets" and cluster definition. This file will be imported into the "CMDB" etcd
- `./confd/dhcpd` this contains the confd configuration for the DHCP server
- `./confd/terraform` this contains the confd configuration which is needed to render the `cluster.tf`.
- `./terraform/modules` contains the `kubernetes` and `profile` modules for deploying Kubernetes
- `./terraform/modules/kubernetes` contains the groups and profile terraform templates to create a Kubernetes cluster the groups will reference to the profiles in `./terraform/modules/profiles`
- `./terraform/modules/profiles` this folder contains all configuration needed to deploy the actual nodes (e.g. master oder worker node)
- `kube-deploy` contains Kubernetes descriptors for calico and kube_proxy

## Starting Matchbox

```bash
make env-file
make pxe_server
```

This will take a while until all components are successfully installed. In the mean time take a look at the provisioning script and the `Makefile`.

## Render Matchbox configuration

```bash
make terraform_config
```

This will copy all files from `./terraform` onto the Matchbox VM (into the folder `/etc/terraform`). In the next step the terraform configuration will be applied against the Matchbox. This step actually renders all the ignition templates and sets the groups according to the Matchbox selectors (we will get back to selectors in a later step). In the end we run two `curl` commands to validate that the configuration was applied successfully (since terraform can't tell us all errors e.g. a missing or bad configured ignition template). The final configuration for terraform will be generated by confd on the Matchbox with the `confd-terraform.service`.

## Deploy the new hosts

```bash
make cluster
```

This command will spin up two "blank" VM's that PXE boot from the Matchbox. Currently there is an issue with PXE boot, Vagrant and the VirtualBox Provider (because of this we need to set an Vagrant box image even we don't use it at all).

SSH into the `pxe_server` with `vagrant ssh pxe_server` and look at the following services:

```bash
sudo journalctl -fu confd-dhcp
sudo journalctl -fu isc-dhcp-server
sudo journalctl -fu confd-terraform
sudo docker exec -ti etcd etcdctl -u root:rootpw ls --recursive
```

## Inspect the Matchbox

If you run `sudo docker exec -ti etcd etcdctl -u root:rootpw watch /nodes/kmaster-fluffy-unicorn-az01-001/deploy` on the Matchbox VM you can also see when the newly booted machines are reporting their "status".

## SSH into the new hosts

In order to ssh into the newly provisioned hosts you need to ssh into the `pxe_server` with `vagrant ssh pxe_server` now you can ssh into the other machines:

```bash
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i insecure_private_key core@192.168.1.2
ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i insecure_private_key core@192.168.1.100
```

## Inspect the Machines

You should take a look at the following services:

```bash
sudo journalctl -fu kubelet
# Only on the master
sudo journalctl -fu kube-scheduler
sudo journalctl -fu kube-controller-manager
sudo journalctl -fu kube-apiserver
sudo journalctl -fu etcd
```

## Fetch kubectl config

In order to interact with the Kuberntes cluter from your host run `make setup_kubectl` (take a look into the Makefile if you are interested in what this step does). If you set `export KUBECONFIG=$(pwd)/.kube/config` for your current session you are able to interact with with cluster. You can validate that everything works with `kubectl get nodes`.

## Deploy kube-proxy and networking

In order to get a fully functioning cluster we need to deploy `kube-proxy` and a [networking solution](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model) in our case we will deploy [Calico](https://docs.projectcalico.org/v3.1/introduction). The Kubernetes configuration files can be found on `./kube-deploy` and can be applied with: `kubectl apply -f kube-deploy/kube-proxy` and `kubectl apply -f kube-deploy/calico`.

## On the master node

If you are on the master node you can read the content of the etcd "cluster" with the following content:

```bash
sudo ETCDCTL_API=3 etcdctl --cacert="/etc/etcd/ca.pem" --key="/etc/etcd/key.pem" --cert="/etc/etcd/crt.pem" get --prefix=true /registry
```

## Clean up

In order to clean everything up: `make clean`
